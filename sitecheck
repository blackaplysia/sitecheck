#!/usr/bin/env python3

import chardet
import difflib
import errno
import hashlib
import json
import os
import requests
import shutil
import subprocess
import sys
from subprocess import PIPE

no_warning = False

def print_info(msg):
    print(msg, file=sys.stderr)

def print_warning(msg):
    if no_warning is False:
        print(msg, file=sys.stderr)

def print_error(msg):
    print(msg, file=sys.stderr)

def shell(cmdline, intext):
    proc = subprocess.Popen(cmdline, shell=True, stdin=PIPE, stdout=PIPE)
    out, err = proc.communicate(input=intext.encode())
    if len(out) < 0:
        return None

    coding = chardet.detect(out)["encoding"]
    if coding is None:
        return None
    
    return out.decode(coding)

class WebChecker:

    def __init__(self):
        self.summaries = {}
        self.contents = {}
        self.diffs = {}
        self.cache_dir = os.path.expanduser("~") + "/.cache/" + os.path.basename(__file__)
        self.cache_backup_dir = self.cache_dir + ".bak"
        self.files_dir = self.cache_dir + "/files"
        self.current_dir = "."
        os.makedirs(self.cache_dir, mode=0o777, exist_ok=True)
        os.makedirs(self.files_dir, mode=0o777, exist_ok=True)
        self.summaries_path = self.cache_dir + "/summaries.json"
        self.diffs_path = self.current_dir + "/diffs.log"
        if os.path.isdir(self.cache_dir):
            if os.path.isdir(self.cache_backup_dir):
                shutil.rmtree(self.cache_backup_dir)
            shutil.copytree(self.cache_dir, self.cache_backup_dir)

    def get_resid(self, name, url):
        resid = hashlib.md5(url.encode()).hexdigest()
        return resid

    def get_resid_by_name(self, name):
        for resid, summary in self.summaries.items():
            if summary.get("name") == name:
                return resid
        else:
            return None

    def update_summary(self, resid, key, value):
        self.summaries[resid].update({ key: value })

    def get_summary_value(self, resid, key):
        return self.summaries[resid].get(key)

    def init_summary(self, resid, name = None, url = None, status = -1):
        if name is None:
            self.update_summary(resid, "status", status)
        else:
            self.summaries.update({ resid: {}})
            self.update_summary(resid, "name", name)
            self.update_summary(resid, "url", url)
            self.update_summary(resid, "status", status)

    def update_contents(self, resid, text, updated = True):
        self.contents[resid].update({ "updated": updated, "text": text})

    def init_contents(self, resid, text = None):
        self.contents.update({ resid: {}})
        self.update_contents(resid, text, updated=False)

    def list(self):
        for resid, summary in self.summaries.items():
            name = summary.get("name")
            url = summary.get("url")
            print("{} {} {}".format(resid, name, url))

    def load(self, site_list, deleting_list):
        try:
            with open(self.summaries_path) as fcache:
                self.summaries = json.loads(fcache.read())
        except Exception as e:
            print_warning("Filed to load summary. {}".format(e))

        for resid in self.summaries:
            self.update_summary(resid, "status", -1)
            try:
                with open(self.files_dir + "/" + resid) as fc:
                    contents = fc.read()
            except Exception as e:
                print_warning("Filed to load {}: {}".format(self.get_summary_value(resid, "name"), e))
            self.init_contents(resid, contents)

        if site_list is not None:
            for site in site_list:
                if len(site) < 2:
                    continue

                name = None
                url = None
                if isinstance(site, list):
                    name = site[0].strip()
                    url = site[1].strip()
                elif isinstance(site, dict):
                    name = site.get("name").strip()
                    url = site.get("url").strip()
                else:
                    continue

                if name is None or len(name) < 1:
                    continue
                if url is None or len(url) < 1:
                    continue

                resid = self.get_resid(name, url)
                if resid not in self.summaries:
                    print_info("Added {}. {}".format(name, url))
                    self.init_summary(resid, name, url, -1)
                    self.init_contents(resid)

        if deleting_list is not None:
            for name in deleting_list:
                resid = self.get_resid_by_name(name)
                if resid is not None:
                    name = self.get_summary_value(resid, "name")
                    url = self.get_summary_value(resid, "url")
                    print_info("Removed {}. {}".format(name, url))
                    del self.summaries[resid]
                    del self.contents[resid]

    def save(self):
        if len(self.summaries) > 0:
            try:
                with open(self.summaries_path, "w", encoding="utf-8") as fcache:
                    json.dump(self.summaries, fcache, ensure_ascii=False, indent=4)
            except Exception as e:
                print_warning("Filed to save summary. {}".format(e))

            for resid, cache in self.contents.items():
                if cache["updated"] is True:
                    try:
                        with open(self.files_dir + "/" + resid, "w") as fc:
                            fc.write(cache["text"])
                    except Exception as e:
                        print_warning("Filed to save {}: {}".format(self.get_summary_value(resid, "name"), e))

                if len(self.diffs) > 0:
                    try:
                        with open(self.diffs_path, "w", encoding="utf-8") as fdiffs:
                            sep = None
                            for resid, diff in self.diffs.items():
                                name = diff.get("name")
                                url = diff.get("url")
                                log = diff.get("log")
                                if sep is not None:
                                    fdiffs.write(sep)
                                fdiffs.write("{} ({})\n".format(name, resid))
                                fdiffs.write("{}\n".format(url))
                                fdiffs.write("{}\n".format(log))
                                sep = "-" * 64 + "\n"
                    except Exception as e:
                        print_warning("Failed to save {}. {}".format(self.diffs_path, e))

    def render(self, html):
        return shell("w3m -T text/html -dump", html)

    def update(self):
        self.diffs = {}
        for resid, summary in self.summaries.items():
            name = summary["name"]
            url = summary["url"]
            res = None
            try:
                res = requests.get(url)
            except requests.exceptions.RequestException as e:
                print_warning("Failed to fetch {}. ({})".format(name, e))
                continue
            except Exception as e:
                print_warning("Failed to fetch {}. ({})".format(name, e))
                continue

            self.update_summary(resid, "status", res.status_code)
            if res.status_code >= 400:
                print_warning("Failed to fetch {}. (status={})".format(name, res.status_code))
            else:
                res.encoding = res.apparent_encoding
                self.update_summary(resid, "encoding", res.encoding)
                md5 = hashlib.md5(res.text.encode()).hexdigest()
                if "hash" not in summary or md5 != summary["hash"]:
                    self.diffs.update({ resid: { "name": name, "url": url }})
                    if self.contents[resid]["text"] is None:
                        self.diffs[resid].update({ "log": "Initial update." })
                    else:
                        old_lines = self.render(self.contents[resid]["text"]).splitlines()
                        new_lines = self.render(res.text).splitlines()
                        diff_lines = difflib.context_diff(old_lines, new_lines)
                        diff = []
                        for l in diff_lines:
                            if l[0] == "+" or l[0] == "!":
                                diff.append(l)
                        self.diffs[resid].update({ "log": "\n".join(diff) })
                    self.update_summary(resid, "hash", md5)
                    self.update_contents(resid, res.text)
                    print_info("Updated {}. {}".format(name, url))
                if "last-modified" in res.headers:
                    last_modified = res.headers["last-modified"]
                    self.update_summary(resid, "last-modefied", last_modified)

if __name__ == "__main__":

    import argparse
    import csv
    from argparse import HelpFormatter
    from operator import attrgetter
    class SortingHelpFormatter(HelpFormatter):
        def add_arguments(self, actions):
            actions = sorted(actions, key=attrgetter("option_strings"))
            super(SortingHelpFormatter, self).add_arguments(actions)

    parser = argparse.ArgumentParser(description="Check updating of web sites", usage="%(prog)s [options] [target]", formatter_class=SortingHelpFormatter)
    group = parser.add_mutually_exclusive_group()
    group.add_argument("-c", "--check", action="store_true", help="check sites")
    group.add_argument("-l", "--list", action="store_true", help="list target sites")
    group.add_argument("-e", "--export", dest="expfile", metavar="FILE", default=None, help="export site list to file")
    group.add_argument("-i", "--import", dest="impfile", metavar="FILE", default=None, help="import site list from file")
    group.add_argument("-a", "--add", dest="new_site", metavar="NAME=URL", action="append", help="add an site")
    group.add_argument("-d", "--delete", dest="deleting_site", metavar="NAME", action="append", help="delete an site")

    if len(sys.argv) == 1:
        print_error(parser.format_usage())
        exit(0)

    args = parser.parse_args()

    config = None
    lines = None
    if args.impfile is not None:
        if os.path.exists(args.impfile):
            try:
                with open(args.impfile) as fconfig:
                    lines = fconfig.read() + "\n"
            except Exception as e:
                print_error("Failed to read config. {}".format(e))
                exit(errno.ENOENT)

    if args.new_site is not None:
        lines = ""
        for si in args.new_site:
            lines = lines + si + "\n"

    if lines is not None:
        config = csv.reader(filter(lambda x: len(x.strip()) > 1 and x[0] != "#", lines.splitlines()), delimiter="=", quotechar="\"", doublequote=False, escapechar="\\")

    checker = WebChecker()
    checker.load(config, args.deleting_site)

    if args.expfile is not None:
        try:
            with open(args.expfile, "w", newline="") as fexp:
                writer = csv.writer(fexp, delimiter="=", quotechar="\"", doublequote=False, escapechar="\\")
                for resid in checker.summaries:
                    name = checker.get_summary_value(resid, "name")
                    url = checker.get_summary_value(resid, "url")
                    writer.writerow([name, url])
        except Exception as e:
            print_error("Failed to write config. {}".format(e))
            exit(errno.ENOENT)
    elif args.list is True:
        checker.list()
    elif args.check is True:
        checker.update()
        checker.save()
    else:
        checker.save()
