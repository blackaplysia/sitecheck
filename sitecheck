#!/usr/bin/env python3

import chardet
import difflib
import errno
import hashlib
import json
import logging
import os
import re
import requests
import shutil
import subprocess
import sys
import time
from datetime import datetime
from subprocess import PIPE
from bs4 import BeautifulSoup
from urllib.parse import urljoin

logger = logging.getLogger()
logger.setLevel(logging.DEBUG)

class WebChecker:

    def __init__(self):
        self.properties = {}
        self.summaries = {}
        self.contents = {}
        self.names = {}
        self.isotimestamps ={}
        self.deleting_list = []
        self.cache_dir = os.path.expanduser("~") + "/.cache/" + os.path.basename(__file__)
        self.files_dir = self.cache_dir + "/files"
        self.current_dir = "."
        os.makedirs(self.cache_dir, mode=0o777, exist_ok=True)
        os.makedirs(self.files_dir, mode=0o777, exist_ok=True)
        self.data_path = self.cache_dir + "/" + os.path.basename(__file__) + ".json"
        self.now = time.time()
        self.is_loaded = False
        self.dirty_flag = False

    def get_resid(self, name, url):
        resid = hashlib.md5(url.encode()).hexdigest()
        return resid

    def get_resid_by_name(self, name):
        for resid, summary in self.summaries.items():
            if summary.get("name") == name:
                return resid
        else:
            return None

    def get_resid_list(self, cond = None):
        resids = []
        if cond is None:
            resids = self.summaries.keys()
        elif cond.lower() == "all":
            resids = self.summaries.keys()
        elif cond.lower() == "last":
            if len(self.isotimestamps) > 0:
                s = sorted(self.isotimestamps, reverse=True)
                if len(s) > 0:
                    resids.extend(self.isotimestamps.get(s[0]))
        else:
            for name, resid in self.names.items():
                if cond in name:
                    resids.append(resid)
            for timestamp in self.isotimestamps:
                if cond in timestamp:
                    resids.extend(self.isotimestamps.get(timestamp))
        return resids

    def update_summary(self, resid, key, value):
        self.summaries[resid].update({ key: value })

    def get_summary_value(self, resid, key):
        return self.summaries[resid].get(key)

    def get_property_value(self, key):
        return self.properties.get(key)

    def init_summary(self, resid, name = None, url = None, status = -1):
        if name is None:
            self.update_summary(resid, "status", status)
        else:
            self.summaries.update({ resid: {}})
            self.update_summary(resid, "name", name)
            self.update_summary(resid, "url", url)
            self.update_summary(resid, "status", status)

    def update_contents(self, resid, text, updated = True):
        self.contents[resid].update({ "updated": updated, "text": text})

    def init_contents(self, resid, text = None):
        self.contents.update({ resid: {}})
        self.update_contents(resid, text, updated=False)

    def load(self, site_list = None, deleting_name_list = None, renaming_name_pair_list = None):
        if self.is_loaded == True:
            return

        try:
            with open(self.data_path) as fcache:
                data = json.loads(fcache.read())
                self.properties = data.get("property")
                self.summaries = data.get("summary")
        except FileNotFoundError as e:
            logger.warning("Found no summary")
            logger.debug(e)
        except Exception as e:
            logger.warning("Failed to load summary")
            logger.debug(e)

        self.is_loaded = True

        for resid in self.summaries:
            if self.get_summary_value(resid, "hash") is None:
                self.init_contents(resid)
            else:
                try:
                    with open(self.files_dir + "/" + resid) as fc:
                        contents = fc.read()
                except Exception as e:
                    logger_warning("Failed to load {}".format(self.get_summary_value(resid, "name")))
                    logger_debug(e)
                self.init_contents(resid, contents)

        if site_list is not None:
            for site in site_list:
                if len(site) < 2:
                    continue

                name = None
                url = None
                if isinstance(site, list):
                    name = site[0].strip()
                    url = site[1].strip()
                elif isinstance(site, dict):
                    name = site.get("name").strip()
                    url = site.get("url").strip()
                else:
                    continue

                if name is None or len(name) < 1:
                    continue
                if url is None or len(url) < 1:
                    continue

                resid = self.get_resid(name, url)
                if resid in self.summaries:
                    logger.warning("{} is already registered".format(url))
                else:
                    logger.info("Added {} {}".format(name, url))
                    self.init_summary(resid, name, url, -1)
                    self.init_contents(resid)
                    self.dirty_flag = True

        if deleting_name_list is not None:
            for name in deleting_name_list:
                resid = self.get_resid_by_name(name)
                if resid is not None:
                    name = self.get_summary_value(resid, "name")
                    url = self.get_summary_value(resid, "url")
                    logger.info("Removed {} {}".format(name, url))
                    del self.summaries[resid]
                    del self.contents[resid]
                    self.deleting_list.append(resid)
                    self.dirty_flag = True

        if renaming_name_pair_list is not None:
            for name_pair in renaming_name_pair_list:
                if len(name_pair) < 2:
                    continue

                old = None
                new = None
                if isinstance(name_pair, list):
                    old = name_pair[0].strip()
                    new = name_pair[1].strip()
                elif isinstance(name_pair, dict):
                    old = name_pair.get("old").strip()
                    new = name_pair.get("new").strip()
                else:
                    continue

                if old is None or len(old) < 1:
                    continue
                if new is None or len(new) < 1:
                    continue

                resid = self.get_resid_by_name(old)
                if resid is None:
                    logger.warning("{} is not registered".format(old))
                else:
                    logger.info("Renamed {} to {}".format(old, new))
                    self.update_summary(resid, "name", new)
                    self.dirty_flag = True

        for resid, summary in self.summaries.items():
            name = summary.get("name")
            updated = summary.get("updated")
            isotimestamp = None
            if updated is not None:
                isotimestamp = datetime.utcfromtimestamp(updated).isoformat()
            self.names.update({ name:  resid })
            if isotimestamp is not None:
                if isotimestamp not in self.isotimestamps:
                    self.isotimestamps.update({ isotimestamp: [resid] })
                else:
                    self.isotimestamps[isotimestamp].append(resid)

    def save(self):
        self.load()
        if self.dirty_flag is True:
            if len(self.summaries) > 0:
                self.properties["updated"] = self.now
                if os.path.isdir(self.cache_dir):
                    cache_backup_dir = self.cache_dir + ".bak"
                    if os.path.isdir(cache_backup_dir):
                        shutil.rmtree(cache_backup_dir)
                    shutil.copytree(self.cache_dir, cache_backup_dir)
                try:
                    with open(self.data_path, "w", encoding="utf-8") as fcache:
                        data = { "property": self.properties, "summary": self.summaries }
                        json.dump(data, fcache, ensure_ascii=False, indent=4)
                except Exception as e:
                    logger.warning("Failed to save summary")
                    logger.debug(e)

                for resid, cache in self.contents.items():
                    if cache["updated"] is True:
                        try:
                            with open(self.files_dir + "/" + resid, "w") as fc:
                                fc.write(cache["text"])
                        except Exception as e:
                            logger.warning("Failed to save {}".format(self.get_summary_value(resid, "name")))
                            logger.debug(e)

                for resid in self.deleting_list:
                    contents = self.files_dir + "/" + resid
                    if os.path.exists(contents):
                        os.remove(contents)

    def list(self, cond = None):
        self.load()
        if len(self.summaries) <= 0:
            logger.warning("No sites were found")
        else:
            for resid in sorted(self.get_resid_list(cond), key=lambda resid:self.get_summary_value(resid, "name")):
                name = self.get_summary_value(resid, "name")
                url = self.get_summary_value(resid, "url")
                updated = self.get_summary_value(resid, "updated")
                if updated is not None:
                    updated = datetime.utcfromtimestamp(updated).isoformat()
                print("{} {} {} {}".format(resid, updated, name, url))

    def list_timestamps(self):
        self.load()
        if len(self.summaries) <= 0:
            logger.warning("No sites were found")
        else:
            for t, resids in sorted(self.isotimestamps.items()):
                for resid in sorted(resids, key=lambda resid:self.get_summary_value(resid, "name")):
                    print("{} {}".format(t, self.get_summary_value(resid, "name")))

    def print(self, cond = None):
        self.load()
        if len(self.summaries) <= 0:
            logger.warning("No sites were found")
        else:
            for resid in sorted(self.get_resid_list(cond), key=lambda resid:self.get_summary_value(resid, "name")):
                log = self.get_summary_value(resid, "log")
                if log is None or len(log) == 0:
                    name = self.get_summary_value(resid, "name")
                    logger.warning("Found no content updates: {}".format(name))
                else:
                    print(self.get_summary_value(resid, "log"))

    def get_real_title(self, title):
        real_title = title["name"]
        url = title["link"]
        res = None
        try:
            res = requests.get(url)
        except requests.exceptions.InvalidSchema as e:
            logger.debug(e)
        except requests.exceptions.RequestException as e:
            logger.warning("Failed to fetch {}".format(url))
            logger.debug(e)
        except Exception as e:
            logger.warning("Failed to fetch {}".format(url))
            logger.debug(e)

        if res is not None:
            if res.status_code >= 400:
                logger.warning("Failed to fetch {}. Status code={}".format(url, res.status_code))
            else:
                enc = res.encoding if res.encoding != "ISO-8859-1" else None
                bs = BeautifulSoup(res.content, "html.parser", from_encoding=enc)
                s = None
                s_tag = bs.find("title")
                s_ogp = bs.find("meta", attrs={"property": "og:title"})
                if s_ogp is not None:
                    s = s_ogp.get("content")
                elif s_tag is not None:
                    s = s_tag.get_text()
                if s is not None and len(s) > 0:
                    real_title = s

        return real_title + (" " if len(real_title) > 0 else "") + "---- " + url

    def list_links(self, html, url_base):
        links = []
        titles = {}
        for t in BeautifulSoup(html, "html.parser").find_all("a"):
            ref = t.get("href")
            if ref is not None:
                ref = "".join(filter(lambda c: c >= ' ', ref))
                if url_base is not None:
                    ref = urljoin(url_base, ref)
                cs = t.strings
                if cs is not None:
                    title = re.sub("[ã€€ ]+", " ", "::".join(filter(lambda x: len(x) > 0, [s.strip() for s in cs])))
                tagname = title + (" " if len(title) > 0 else "") + "---- " + ref
                links.append(tagname)
                titles.update({tagname: { "name": title, "link": ref }})
        return (links, titles)

    def update(self, rerun = False):
        self.load()
        if len(self.summaries) <= 0:
            logger.warning("No sites were found")
        else:
            for resid, summary in self.summaries.items():
                status = summary.get("status")
                if rerun is True and status > 0 and status < 400:
                    continue

                self.update_summary(resid, "status", -1)

                name = summary["name"]
                url = summary["url"]
                res = None
                try:
                    res = requests.get(url)
                except requests.exceptions.RequestException as e:
                    logger.warning("Failed to fetch {}".format(name))
                    logger.debug(e)
                    continue
                except Exception as e:
                    logger.warning("Failed to fetch {}".format(name))
                    logger.debug(e)
                    continue

                self.update_summary(resid, "status", res.status_code)
                if res.status_code >= 400:
                    logger.warning("Failed to fetch {}. Status code={}".format(name, res.status_code))
                else:
                    res.encoding = res.apparent_encoding
                    self.update_summary(resid, "encoding", res.encoding)
                    md5 = hashlib.md5(res.text.encode()).hexdigest()
                    if "hash" not in summary or md5 != summary["hash"]:
                        diff = ""
                        if self.contents[resid]["text"] is not None:
                            (old_lines, old_titles) = self.list_links(self.contents[resid]["text"], url)
                            (new_lines, new_titles) = self.list_links(res.text, url)
                            diff_lines = difflib.unified_diff(old_lines, new_lines, n=0)
                            updated_list = list(map(lambda x: self.get_real_title(new_titles[x[1:]]), filter(lambda l: l[0] == "+" and not re.match("^\++\s$", l), diff_lines)))
                            if len(updated_list) > 0:
                                diff = "\n".join(sorted(set(updated_list), key=updated_list.index))
                        self.update_summary(resid, "log", diff)
                        self.update_summary(resid, "hash", md5)
                        self.update_contents(resid, res.text)
                        self.update_summary(resid, "updated", self.now)
                        logger.info("Updated {}".format(name))
                        self.dirty_flag = True

    def sites(self, cond = None):
        self.load()
        self.iter_list = []
        for resid in self.get_resid_list(cond):
            name = self.get_summary_value(resid, "name")
            url = self.get_summary_value(resid, "url")
            self.iter_list.append([name, url])
        self.iter_current = 0
        return self

    def __iter__(self):
        return self

    def __next__(self):
        if self.iter_current == len(self.iter_list):
            raise StopIteration()
        v = self.iter_list[self.iter_current]
        self.iter_current += 1
        return v

if __name__ == "__main__":

    import argparse
    import csv
    from argparse import HelpFormatter
    from operator import attrgetter
    class SortingHelpFormatter(HelpFormatter):
        def add_arguments(self, actions):
            actions = sorted(actions, key=attrgetter("option_strings"))
#            actions = sorted(actions, key=lambda x: x.option_strings[0].lstrip("-").lower())
            super(SortingHelpFormatter, self).add_arguments(actions)

    parser = argparse.ArgumentParser(description="Check updating of web sites", formatter_class=SortingHelpFormatter)
    parser.add_argument("--debug", action="store_true", help="debug output")

    subparsers = parser.add_subparsers(dest="subparser_name", title="action arguments")
    subparser_config = subparsers.add_parser("config", help="configure the command")
    subparser_config.add_argument("--import", dest="impfile", metavar="FILE", help="import site list from file")
    subparser_config.add_argument("--export", dest="expfile", metavar="FILE", help="export site list to file")
    subparser_config.add_argument("--add", nargs=2, metavar=("NAME", "URL"), action="append", help="add a site")
    subparser_config.add_argument("--delete", metavar="NAME", action="append", help="delete a site")
    subparser_config.add_argument("--rename", nargs=2, metavar=("OLD", "NEW"), action="append", help="rename a site")
    subparser_check = subparsers.add_parser("check", help="check registered sites")
    subparser_recheck = subparsers.add_parser("recheck", help="rerun to check registered sites")
    subparser_timestamps = subparsers.add_parser("timestamps", help="print last updated timestamps")
    subparser_list = subparsers.add_parser("list", help="print site list")
    subparser_list.add_argument("target", nargs="?", metavar="TARGET", help="target (ALL|LAST|name|time)")
    subparser_print = subparsers.add_parser("print", help="print last updates")
    subparser_print.add_argument("target", nargs="?", metavar="TARGET", help="target (ALL|LAST|name|time)")

    if len(sys.argv) == 1:
        print(parser.format_usage(), file=sys.stderr)
        exit(0)

    args = parser.parse_args()

    log_format = "%(levelname)s: %(message)s"
    log_level = logging.INFO
    if args.debug is True:
        log_format = "%(asctime)s %(name)-12s %(levelname)-8s %(message)s"
        log_level = logging.DEBUG
    console = logging.StreamHandler()
    console.setFormatter(logging.Formatter(log_format))
    console.setLevel(log_level)
    logger.addHandler(console)

    checker = WebChecker()

    if args.subparser_name == "check" or args.subparser_name == "c":
        checker.update()
    elif args.subparser_name == "recheck" or args.subparser_name == "r":
        checker.update(rerun=True)
    elif args.subparser_name == "timestamps" or args.subparser_name == "ts":
        checker.list_timestamps()
    elif args.subparser_name == "list" or args.subparser_name == "l":
        checker.list(args.target)
    elif args.subparser_name == "print" or args.subparser_name == "p":
        checker.print(args.target)
    elif args.subparser_name == "config" or args.subparser_name == "cfg":
        site_list = []
        lines = None
        if args.impfile is not None:
            fimp = sys.stdin
            try:
                if args.impfile != "-":
                    fimp = open(args.impfile)
                lines = fimp.read() + "\n"
                if fimp != sys.stdin:
                    fimp.close()
            except Exception as e:
                logger.critical("Failed to import config")
                logger.debug(e)
                exit(errno.ENOENT)

        if lines is not None:
            config = csv.reader(filter(lambda x: len(x.strip()) > 1 and x[0] != "#", lines.splitlines()), delimiter="=", quotechar="\"", doublequote=False, escapechar="\\")
            for site in config:
                site_list.append(site)

        if args.add is not None:
            for site in args.add:
                site_list.append(site)

        checker.load(site_list, args.delete, args.rename)

        if args.expfile is not None:
            fexp = sys.stdout
            if args.expfile != "-":
                fexp = open(args.expfile, "w", newline="")
            try:
                writer = csv.writer(fexp, delimiter="=", quotechar="\"", doublequote=False, escapechar="\\")
                for v in checker.sites():
                    writer.writerow(v)
                if fexp != sys.stdout:
                    fexp.close()
            except Exception as e:
                logger.critical("Failed to export config")
                logger.debug(e)
                exit(errno.ENOENT)

    checker.save()
