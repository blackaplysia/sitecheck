#!/usr/bin/env python3

import chardet
import difflib
import errno
import hashlib
import json
import os
import requests
import shutil
import subprocess
import sys
import time
from datetime import datetime
from subprocess import PIPE

no_warning = False

def print_info(msg):
    print(msg, file=sys.stderr)

def print_warning(msg):
    if no_warning is False:
        print(msg, file=sys.stderr)

def print_error(msg):
    print(msg, file=sys.stderr)

def shell(cmdline, intext):
    proc = subprocess.Popen(cmdline, shell=True, stdin=PIPE, stdout=PIPE)
    out, err = proc.communicate(input=intext.encode())
    if len(out) < 0:
        return None

    coding = chardet.detect(out)["encoding"]
    if coding is None:
        return None
    
    return out.decode(coding)

class WebChecker:

    def __init__(self):
        self.properties = {}
        self.summaries = {}
        self.contents = {}
        self.deleting_list = []
        self.cache_dir = os.path.expanduser("~") + "/.cache/" + os.path.basename(__file__)
        self.files_dir = self.cache_dir + "/files"
        self.current_dir = "."
        os.makedirs(self.cache_dir, mode=0o777, exist_ok=True)
        os.makedirs(self.files_dir, mode=0o777, exist_ok=True)
        self.data_path = self.cache_dir + "/" + os.path.basename(__file__) + ".json"
        self.now = time.time()
        self.dirty_flag = False

    def get_resid(self, name, url):
        resid = hashlib.md5(url.encode()).hexdigest()
        return resid

    def get_resid_by_name(self, name):
        for resid, summary in self.summaries.items():
            if summary.get("name") == name:
                return resid
        else:
            return None

    def update_summary(self, resid, key, value):
        self.summaries[resid].update({ key: value })

    def get_summary_value(self, resid, key):
        return self.summaries[resid].get(key)

    def get_property_value(self, key):
        return self.properties.get(key)

    def init_summary(self, resid, name = None, url = None, status = -1):
        if name is None:
            self.update_summary(resid, "status", status)
        else:
            self.summaries.update({ resid: {}})
            self.update_summary(resid, "name", name)
            self.update_summary(resid, "url", url)
            self.update_summary(resid, "status", status)

    def update_contents(self, resid, text, updated = True):
        self.contents[resid].update({ "updated": updated, "text": text})

    def init_contents(self, resid, text = None):
        self.contents.update({ resid: {}})
        self.update_contents(resid, text, updated=False)

    def list(self, time_isoformat = None):
        for resid, summary in self.summaries.items():
            name = summary.get("name")
            url = summary.get("url")
            updated = summary.get("updated")
            if updated is not None:
                updated = datetime.utcfromtimestamp(updated).isoformat()
            if time_isoformat is None or time_isoformat == updated:
                print("{} {} {} {}".format(resid, updated, name, url))

    def load(self, site_list, deleting_name_list):
        try:
            with open(self.data_path) as fcache:
                data = json.loads(fcache.read())
                self.properties = data.get("property")
                self.summaries = data.get("summary")
        except Exception as e:
            print_warning("Filed to load summary. {}".format(e))

        for resid in self.summaries:
            if self.get_summary_value(resid, "hash") is None:
                self.init_contents(resid)
            else:
                try:
                    with open(self.files_dir + "/" + resid) as fc:
                        contents = fc.read()
                except Exception as e:
                    print_warning("Filed to load {}: {}".format(self.get_summary_value(resid, "name"), e))
                self.init_contents(resid, contents)

        if site_list is not None:
            for site in site_list:
                if len(site) < 2:
                    continue

                name = None
                url = None
                if isinstance(site, list):
                    name = site[0].strip()
                    url = site[1].strip()
                elif isinstance(site, dict):
                    name = site.get("name").strip()
                    url = site.get("url").strip()
                else:
                    continue

                if name is None or len(name) < 1:
                    continue
                if url is None or len(url) < 1:
                    continue

                resid = self.get_resid(name, url)
                if resid not in self.summaries:
                    print_info("Added {}. {}".format(name, url))
                    self.init_summary(resid, name, url, -1)
                    self.init_contents(resid)
                    self.dirty_flag = True

        if deleting_name_list is not None:
            for name in deleting_name_list:
                resid = self.get_resid_by_name(name)
                if resid is not None:
                    name = self.get_summary_value(resid, "name")
                    url = self.get_summary_value(resid, "url")
                    print_info("Removed {}. {}".format(name, url))
                    del self.summaries[resid]
                    del self.contents[resid]
                    self.deleting_list.append(resid)
                    self.dirty_flag = True

    def save(self):
        if self.dirty_flag is False:
            print("Nothing is done")
            return

        if len(self.summaries) > 0:
            self.properties["updated"] = self.now
            if os.path.isdir(self.cache_dir):
                cache_backup_dir = self.cache_dir + ".bak"
                if os.path.isdir(cache_backup_dir):
                    shutil.rmtree(cache_backup_dir)
                shutil.copytree(self.cache_dir, cache_backup_dir)
            try:
                with open(self.data_path, "w", encoding="utf-8") as fcache:
                    data = { "property": self.properties, "summary": self.summaries }
                    json.dump(data, fcache, ensure_ascii=False, indent=4)
            except Exception as e:
                print_warning("Filed to save summary. {}".format(e))

            for resid, cache in self.contents.items():
                if cache["updated"] is True:
                    try:
                        with open(self.files_dir + "/" + resid, "w") as fc:
                            fc.write(cache["text"])
                    except Exception as e:
                        print_warning("Filed to save {}: {}".format(self.get_summary_value(resid, "name"), e))

            for resid in self.deleting_list:
                contents = self.files_dir + "/" + resid
                if os.path.exists(contents):
                    os.remove(contents)

    def print(self, time_isoformat = None):
        if time_isoformat is None:
            time_isoformat = datetime.utcfromtimestamp(self.get_property_value("updated")).isoformat()
        if len(self.summaries) > 0:
            sep = None
            for resid, summary in self.summaries.items():
                updated = self.get_summary_value(resid, "updated")
                if updated is not None:
                    updated = datetime.utcfromtimestamp(updated).isoformat()
                if time_isoformat is None or time_isoformat == updated:
                    if sep is not None:
                        print(sep)
                    print(self.get_summary_value(resid, "log"))
                    sep = "-" * 72

    def render(self, html):
        return shell("w3m -T text/html -dump", html)

    def update(self, do_rerun = False):
        for resid, summary in self.summaries.items():
            status = summary.get("status")
            if do_rerun is True and status > 0 and status < 400:
                continue

            self.update_summary(resid, "status", -1)

            name = summary["name"]
            url = summary["url"]
            res = None
            try:
                res = requests.get(url)
            except requests.exceptions.RequestException as e:
                print_warning("Failed to fetch {}. ({})".format(name, e))
                continue
            except Exception as e:
                print_warning("Failed to fetch {}. ({})".format(name, e))
                continue

            self.update_summary(resid, "status", res.status_code)
            if res.status_code >= 400:
                print_warning("Failed to fetch {}. (status={})".format(name, res.status_code))
            else:
                res.encoding = res.apparent_encoding
                self.update_summary(resid, "encoding", res.encoding)
                md5 = hashlib.md5(res.text.encode()).hexdigest()
                if "hash" not in summary or md5 != summary["hash"]:
                    diff_header_lines = []
                    diff_header_lines.append("name: " + name)
                    diff_header_lines.append("url: " + url)
                    diff_header_lines.append("resid: " + resid)
                    diff_header_lines.append("hash: " + md5)
                    diff = "\n".join(diff_header_lines) + "\n"
                    if self.contents[resid]["text"] is None:
                        diff = diff + "Initial update"
                        self.update_summary(resid, "log", diff)
                    else:
                        old_lines = self.render(self.contents[resid]["text"]).splitlines()
                        new_lines = self.render(res.text).splitlines()
                        diff_lines = difflib.context_diff(old_lines, new_lines)
                        for l in diff_lines:
                            if l[0] == "+" or l[0] == "!":
                                diff = diff + "> " + l + "\n"
                        self.update_summary(resid, "log", diff )
                    self.update_summary(resid, "hash", md5)
                    self.update_contents(resid, res.text)
                    self.update_summary(resid, "updated", self.now)
                    print_info("Updated {}. {}".format(name, url))
                    self.dirty_flag = True

if __name__ == "__main__":

    import argparse
    import csv
    from argparse import HelpFormatter
    from operator import attrgetter
    class SortingHelpFormatter(HelpFormatter):
        def add_arguments(self, actions):
            actions = sorted(actions, key=attrgetter("option_strings"))
            super(SortingHelpFormatter, self).add_arguments(actions)

    parser = argparse.ArgumentParser(description="Check updating of web sites", usage="%(prog)s [options] [target]", formatter_class=SortingHelpFormatter)
    parser.add_argument("-i", "--import", dest="impfile", metavar="FILE", default=None, help="import site list from file")
    parser.add_argument("-a", "--add", nargs=2, dest="new_site", metavar="NAME URL", action="append", help="add an site")
    parser.add_argument("-d", "--delete", dest="deleting_site", metavar="NAME", action="append", help="delete an site")

    group = parser.add_mutually_exclusive_group()
    group.add_argument("-c", "--check", action="store_true", help="check sites")
    group.add_argument("-C", "--recheck", action="store_true", help="rerun to check sites that has failed to update")
    group.add_argument("-l", "--list", action="store_true", help="list target sites")
    group.add_argument("-L", "--list-at", metavar="TIME", help="list target sites with last update time")
    group.add_argument("-e", "--export", dest="expfile", metavar="FILE", default=None, help="export site list to file")
    group.add_argument("-p", "--print", action="store_true", help="print the last updates")
    group.add_argument("-P", "--print-at", metavar="TIME", help="print with last update time")

    if len(sys.argv) == 1:
        print_error(parser.format_usage())
        exit(0)

    args = parser.parse_args()

    site_list = []
    lines = None
    if args.impfile is not None:
        try:
            with open(args.impfile) as fconfig:
                lines = fconfig.read() + "\n"
        except Exception as e:
            print_error("Failed to read config. {}".format(e))
            exit(errno.ENOENT)

        if lines is not None:
            config = csv.reader(filter(lambda x: len(x.strip()) > 1 and x[0] != "#", lines.splitlines()), delimiter="=", quotechar="\"", doublequote=False, escapechar="\\")
            for site in config:
                site_list.append(site)

    if args.new_site is not None:
        for site in args.new_site:
            site_list.append(site)

    checker = WebChecker()
    checker.load(site_list, args.deleting_site)

    if args.expfile is not None:
        try:
            with open(args.expfile, "w", newline="") as fexp:
                writer = csv.writer(fexp, delimiter="=", quotechar="\"", doublequote=False, escapechar="\\")
                for resid in checker.summaries:
                    name = checker.get_summary_value(resid, "name")
                    url = checker.get_summary_value(resid, "url")
                    writer.writerow([name, url])
        except Exception as e:
            print_error("Failed to write config. {}".format(e))
            exit(errno.ENOENT)
    elif args.list is True:
        checker.list()
    elif args.list_at is not None:
        checker.list(args.list_at)
    elif args.check is True:
        checker.update()
    elif args.recheck is True:
        checker.update(True)
    elif args.print is True:
        checker.print()
    elif args.print_at is not None:
        checker.print(args.print_at)

    checker.save()
