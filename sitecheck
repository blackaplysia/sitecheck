#!/usr/bin/env python3

import chardet
import difflib
import errno
import hashlib
import json
import logging
import os
import re
import requests
import shutil
import subprocess
import sys
import time
from datetime import datetime
from subprocess import PIPE
from bs4 import BeautifulSoup
from urllib.parse import urljoin

logger = logging.getLogger()

class WebChecker:

    def __init__(self):
        self.properties = {}
        self.summaries = {}
        self.contents = {}
        self.names = {}
        self.isotimestamps ={}
        self.deleting_list = []
        self.cache_dir = os.path.expanduser("~") + "/.cache/" + os.path.basename(__file__)
        self.files_dir = self.cache_dir + "/files"
        self.current_dir = "."
        os.makedirs(self.cache_dir, mode=0o777, exist_ok=True)
        os.makedirs(self.files_dir, mode=0o777, exist_ok=True)
        self.data_path = self.cache_dir + "/" + os.path.basename(__file__) + ".json"
        self.now = time.time()
        self.dirty_flag = False

    def get_resid(self, name, url):
        resid = hashlib.md5(url.encode()).hexdigest()
        return resid

    def get_resid_by_name(self, name):
        for resid, summary in self.summaries.items():
            if summary.get("name") == name:
                return resid
        else:
            return None

    def get_resid_list(self, cond = None):
        resids = []
        if cond is None:
            resids = self.summaries.keys()
        elif cond.lower() == "all":
            resids = self.summaries.keys()
        elif cond.lower() == "last":
            if len(self.isotimestamps) > 0:
                s = sorted(self.isotimestamps, reverse=True)
                if len(s) > 0:
                    resids.extend(self.isotimestamps.get(s[0]))
        elif cond in self.names:
            resids.append(self.names.get(cond))
        elif cond in self.isotimestamps:
            resids.extend(self.isotimestamps.get(cond))
        return resids

    def update_summary(self, resid, key, value):
        self.summaries[resid].update({ key: value })

    def get_summary_value(self, resid, key):
        return self.summaries[resid].get(key)

    def get_property_value(self, key):
        return self.properties.get(key)

    def init_summary(self, resid, name = None, url = None, status = -1):
        if name is None:
            self.update_summary(resid, "status", status)
        else:
            self.summaries.update({ resid: {}})
            self.update_summary(resid, "name", name)
            self.update_summary(resid, "url", url)
            self.update_summary(resid, "status", status)

    def update_contents(self, resid, text, updated = True):
        self.contents[resid].update({ "updated": updated, "text": text})

    def init_contents(self, resid, text = None):
        self.contents.update({ resid: {}})
        self.update_contents(resid, text, updated=False)

    def load(self, site_list, deleting_name_list):
        try:
            with open(self.data_path) as fcache:
                data = json.loads(fcache.read())
                self.properties = data.get("property")
                self.summaries = data.get("summary")
        except Exception as e:
            logger.warning("Failed to load summary")
            logger.debug(e)

        for resid in self.summaries:
            if self.get_summary_value(resid, "hash") is None:
                self.init_contents(resid)
            else:
                try:
                    with open(self.files_dir + "/" + resid) as fc:
                        contents = fc.read()
                except Exception as e:
                    logger_warning("Failed to load {}".format(self.get_summary_value(resid, "name")))
                    logger_debug(e)
                self.init_contents(resid, contents)

        if site_list is not None:
            for site in site_list:
                if len(site) < 2:
                    continue

                name = None
                url = None
                if isinstance(site, list):
                    name = site[0].strip()
                    url = site[1].strip()
                elif isinstance(site, dict):
                    name = site.get("name").strip()
                    url = site.get("url").strip()
                else:
                    continue

                if name is None or len(name) < 1:
                    continue
                if url is None or len(url) < 1:
                    continue

                resid = self.get_resid(name, url)
                if resid not in self.summaries:
                    logger.info("Added {} {}".format(name, url))
                    self.init_summary(resid, name, url, -1)
                    self.init_contents(resid)
                    self.dirty_flag = True

        if deleting_name_list is not None:
            for name in deleting_name_list:
                resid = self.get_resid_by_name(name)
                if resid is not None:
                    name = self.get_summary_value(resid, "name")
                    url = self.get_summary_value(resid, "url")
                    logger.info("Removed {} {}".format(name, url))
                    del self.summaries[resid]
                    del self.contents[resid]
                    self.deleting_list.append(resid)
                    self.dirty_flag = True

        for resid, summary in self.summaries.items():
            name = summary.get("name")
            updated = summary.get("updated")
            isotimestamp = None
            if updated is not None:
                isotimestamp = datetime.utcfromtimestamp(updated).isoformat()
            self.names.update({ name:  resid })
            if isotimestamp is not None:
                if isotimestamp not in self.isotimestamps:
                    self.isotimestamps.update({ isotimestamp: [resid] })
                else:
                    self.isotimestamps[isotimestamp].append(resid)

    def save(self):
        if self.dirty_flag is True:
            if len(self.summaries) > 0:
                self.properties["updated"] = self.now
                if os.path.isdir(self.cache_dir):
                    cache_backup_dir = self.cache_dir + ".bak"
                    if os.path.isdir(cache_backup_dir):
                        shutil.rmtree(cache_backup_dir)
                    shutil.copytree(self.cache_dir, cache_backup_dir)
                try:
                    with open(self.data_path, "w", encoding="utf-8") as fcache:
                        data = { "property": self.properties, "summary": self.summaries }
                        json.dump(data, fcache, ensure_ascii=False, indent=4)
                except Exception as e:
                    logger.warning("Failed to save summary")
                    logger.debug(e)

                for resid, cache in self.contents.items():
                    if cache["updated"] is True:
                        try:
                            with open(self.files_dir + "/" + resid, "w") as fc:
                                fc.write(cache["text"])
                        except Exception as e:
                            logger.warning("Failed to save {}".format(self.get_summary_value(resid, "name")))
                            logger.debug(e)

                for resid in self.deleting_list:
                    contents = self.files_dir + "/" + resid
                    if os.path.exists(contents):
                        os.remove(contents)

    def list(self, cond = None):
        if len(self.summaries) > 0:
            for resid in self.get_resid_list(cond):
                name = self.get_summary_value(resid, "name")
                url = self.get_summary_value(resid, "url")
                updated = self.get_summary_value(resid, "updated")
                if updated is not None:
                    updated = datetime.utcfromtimestamp(updated).isoformat()
                print("{} {} {} {}".format(resid, updated, name, url))

    def print(self, cond = None):
        if len(self.summaries) > 0:
            sep = None
            for resid in self.get_resid_list(cond):
                updated = self.get_summary_value(resid, "updated")
                if updated is not None:
                    updated = datetime.utcfromtimestamp(updated).isoformat()
                if sep is not None:
                    print(sep)
                print(self.get_summary_value(resid, "log"))
                sep = "-" * 72

    def list_links(self, html, url_base):
        links = []
        for t in BeautifulSoup(html, "html.parser").find_all("a"):
            ref = t.get("href")
            if ref is not None:
                ref = "".join(filter(lambda c: c >= ' ', ref))
                if url_base is not None:
                    ref = urljoin(url_base, ref)
                cs = t.strings
                if cs is not None:
                    tagname = re.sub("[ã€€ ]+", " ", "::".join(filter(lambda x: len(x) > 0, [s.strip() for s in cs])))
                links.append(tagname + (" " if len(tagname) > 0 else "") + "---- " + ref)
        return links

    def update(self, do_rerun = False):
        if len(self.summaries) > 0:
            for resid, summary in self.summaries.items():
                status = summary.get("status")
                if do_rerun is True and status > 0 and status < 400:
                    continue

                self.update_summary(resid, "status", -1)

                name = summary["name"]
                url = summary["url"]
                res = None
                try:
                    res = requests.get(url)
                except requests.exceptions.RequestException as e:
                    logger.warning("Failed to fetch {}".format(name))
                    logger.debug(e)
                    continue
                except Exception as e:
                    logger.warning("Failed to fetch {}".format(name))
                    logger.debug(e)
                    continue

                self.update_summary(resid, "status", res.status_code)
                if res.status_code >= 400:
                    logger.warning("Failed to fetch {}".format(name), "Status code={}".format(res.status_code))
                else:
                    res.encoding = res.apparent_encoding
                    self.update_summary(resid, "encoding", res.encoding)
                    md5 = hashlib.md5(res.text.encode()).hexdigest()
                    if "hash" not in summary or md5 != summary["hash"]:
                        diff_header_lines = []
                        diff_header_lines.append("name: " + name)
                        diff_header_lines.append("url: " + url)
                        diff_header_lines.append("resid: " + resid)
                        diff_header_lines.append("hash: " + md5)
                        diff_header_lines.append("updated: " + datetime.utcfromtimestamp(self.now).isoformat())
                        diff = "\n".join(diff_header_lines) + "\n"
                        if self.contents[resid]["text"] is None:
                            diff = diff + "Initial update"
                            self.update_summary(resid, "log", diff)
                        else:
                            old_lines = self.list_links(self.contents[resid]["text"], url)
                            new_lines = self.list_links(res.text, url)
                            diff_lines = difflib.unified_diff(old_lines, new_lines, n=0)
                            diff += "\n".join(map(lambda x: "> " + x, filter(lambda l: l[0] == "+" and not re.match("^\++\s$", l), diff_lines)))
                            self.update_summary(resid, "log", diff)
                        self.update_summary(resid, "hash", md5)
                        self.update_contents(resid, res.text)
                        self.update_summary(resid, "updated", self.now)
                        logger.info("Updated {}".format(name))
                        self.dirty_flag = True

class SiteVecIterator(object):
    def __init__(self, checker, cond = None):
        self.list = []
        for resid in checker.get_resid_list(cond):
            name = checker.get_summary_value(resid, "name")
            url = checker.get_summary_value(resid, "url")
            self.list.append([name, url])
        self.current = 0

    def __iter__(self):
        return self

    def __next__(self):
        if self.current == len(self.list):
            raise StopIteration()
        v = self.list[self.current]
        self.current += 1
        return v

if __name__ == "__main__":

    import argparse
    import csv
    from argparse import HelpFormatter
    from operator import attrgetter
    class SortingHelpFormatter(HelpFormatter):
        def add_arguments(self, actions):
            actions = sorted(actions, key=attrgetter("option_strings"))
            super(SortingHelpFormatter, self).add_arguments(actions)

    parser = argparse.ArgumentParser(description="Check updating of web sites", formatter_class=SortingHelpFormatter)
    parser.add_argument("-i", dest="impfile", action="append", nargs="?", metavar="FILE", help="import site list from file")
    parser.add_argument("-a", dest="new_site", nargs=2, metavar=("NAME", "URL"), action="append", help="add an site")
    parser.add_argument("-d", dest="deleting_site", metavar="NAME", action="append", help="delete an site")
    parser.add_argument("--log", action="store_true", help="debug output to log file")

    group = parser.add_mutually_exclusive_group()
    group.add_argument("-e", dest="expfile", action="append", nargs="?", metavar="FILE", help="export site list to file")
    group.add_argument("-c", dest="check", action="store_true", help="check sites")
    group.add_argument("-C", dest="recheck", action="store_true", help="rerun to check sites that has failed to update")
    group.add_argument("-l", dest="list", action="append", nargs="?", metavar="COND", help="list target sites (cond: ALL|LAST|name|time)")
    group.add_argument("-p", dest="print", action="append", nargs="?", metavar="COND", help="print the last updates (cond: ALL|LAST|name|time)")

    if len(sys.argv) == 1:
        print(parser.format_usage(), file=sys.stderr)
        exit(0)

    args = parser.parse_args()

    log_file_format = "%(asctime)s %(name)-12s %(levelname)-8s %(message)s"
    log_console_format = "%(levelname)s: %(message)s"
    if args.log is True:
        logging.basicConfig(filename=os.path.basename(__file__) + ".log", format=log_file_format)
        logging.getLogger("chardet.charsetprober").setLevel(level=logging.INFO)
    logger.setLevel(logging.DEBUG)
    console = logging.StreamHandler()
    console.setFormatter(logging.Formatter(log_console_format))
    console.setLevel(logging.INFO)
    logger.addHandler(console)

    site_list = []
    lines = None
    if args.impfile is not None:
        fconfig = sys.stdin
        try:
            fn = args.impfile[0]
            if fn is not None:
                fconfig = open(fn)
            lines = fconfig.read() + "\n"
            if fconfig != sys.stdin:
                fconfig.close()
        except Exception as e:
            logger.critical("Failed to read config")
            logger.debug(e)
            exit(errno.ENOENT)

        if lines is not None:
            config = csv.reader(filter(lambda x: len(x.strip()) > 1 and x[0] != "#", lines.splitlines()), delimiter="=", quotechar="\"", doublequote=False, escapechar="\\")
            for site in config:
                site_list.append(site)

    if args.new_site is not None:
        for site in args.new_site:
            site_list.append(site)

    checker = WebChecker()
    checker.load(site_list, args.deleting_site)

    if args.expfile is not None:
        fexp = sys.stdout
        try:
            fn = args.expfile[0]
            if fn is not None:
                fexp = open(fn, "w", newline="")
            writer = csv.writer(fexp, delimiter="=", quotechar="\"", doublequote=False, escapechar="\\")
            for v in SiteVecIterator(checker):
                writer.writerow(v)
            if fexp != sys.stdout:
                fexp.close()
        except Exception as e:
            logger.critical("Failed to write config")
            logger.debug(e)
            exit(errno.ENOENT)
    elif args.check is True:
        checker.update()
    elif args.recheck is True:
        checker.update(True)
    elif args.list is not None:
        checker.list(args.list[0])
    elif args.print is not None:
        checker.print(args.print[0])

    checker.save()
